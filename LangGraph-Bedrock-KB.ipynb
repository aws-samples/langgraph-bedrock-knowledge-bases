{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph with Amazon Bedrock Knowledge Bases \n",
    "\n",
    "This notebook will show you know to make use of [Langgraph](https://python.langchain.com/docs/langgraph),and [Amazon Bedrock Knowledge bases](https://aws.amazon.com/bedrock/knowledge-bases/) as a RAG source to retrieve relevant documents from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.embeddings import  BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing import Dict, TypedDict\n",
    "import os\n",
    "\n",
    "\n",
    "# setup boto3 config to allow for retrying\n",
    "my_region = \"us-west-2\"\n",
    "my_config = Config(\n",
    "    region_name = my_region,\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 3,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "# setup bedrock runtime client \n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", config = my_config)\n",
    "# setup bedrock agent runtime client\n",
    "bedrock_agent_rt = boto3.client(\"bedrock-agent-runtime\", config = my_config)\n",
    "# setup S3 client\n",
    "s3 = boto3.client(\"s3\", config = my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Bedrock  \n",
    "In this notebook, we will be making use of Anthropic's Calude 3 Sonnet model and the Amazon titan embeddings model. If you would like to use a different model, all the model IDs are available [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"Human\"],\n",
    "}\n",
    "\n",
    "sonnet_llm = ChatBedrock(\n",
    "    client=bedrock_rt,\n",
    "    model_id=sonnet_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\"\n",
    "embedding_llm = BedrockEmbeddings(client = bedrock_rt, model_id = embeddings_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your bedrock knowledge base ID here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=\"<Bedrock KB ID>\",\n",
    "    region_name = my_region,\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create router\n",
    "We will now define the router that will make decision on which RAG or functions to use. In this example we will only include 1 RAG, but feel free to extend this section to add your own tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = \"\"\"rag: Get data about amazon and amazon web services\n",
    "fixed: get data about the IRS or taxes \"\"\"\n",
    "\n",
    "\n",
    "system_msg = f\"\"\"\n",
    "<instructions>\n",
    "You are an assistant that helps users pull data from the correct data source. You pick the correct data source based on the question asked\n",
    "<tools>\n",
    "{data_sources}\n",
    "</tools>\n",
    "\n",
    "You only output the name of the tool\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "input_qn_template = \"\"\"\n",
    "<input question>\n",
    "{question}\n",
    "</input question>\"\"\"\n",
    "\n",
    "route_template = [\n",
    "    (\"system\", system_msg),\n",
    "    (\"user\", input_qn_template),\n",
    "]\n",
    "\n",
    "\n",
    "route_prompt_template = ChatPromptTemplate.from_messages(route_template)\n",
    "\n",
    "router_chain = route_prompt_template | sonnet_llm | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_invoke(state):\n",
    "    print(\"Router invoked. \")\n",
    "    # Invoke the router to pick the correct data source \n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    # run retriever object \n",
    "    route_out = router_chain.invoke({\"question\":question})\n",
    "    return {\"keys\": {\"route\": route_out, \"question\": question}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(state):\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    print(\"Generator invoked\")\n",
    "\n",
    "    # run retriever object \n",
    "    template = [\n",
    "        (\"system\", \"You are a knowledgeable and helpful QnA bot. Your task is to provide accurate and relevant answers to questions asked by users. Use the provided context to answer the questions, and if the context does not contain enough information to answer a question, politely indicate that you do not have enough information.\"),\n",
    "        (\"user\", \"\"\"Use the following pieces of retrieved context to answer the question. \n",
    "         <context> {context} </context>\n",
    "         <question> {question} </question> \n",
    "         Keep the answer concise \"\"\"),\n",
    "    ]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(template)\n",
    "    rag_chain = prompt | sonnet_llm | StrOutputParser()\n",
    "\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "\n",
    "    return {\"keys\": {\"generation\": generation, \"question\": question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  retrieve only\n",
    "def retrieve(state):\n",
    "    print(\"Retriever invoked\")\n",
    "\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    docs = bedrock_retriever.invoke(question)\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        print(doc.page_content)\n",
    "        documents.append(doc.page_content)\n",
    "\n",
    "    print(documents)\n",
    "    return {\"keys\": {\"documents\": documents , \"question\": question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mock fixed data source\n",
    "\n",
    "#  retrieve only\n",
    "def fixed_data(state):\n",
    "    print(\"Fixed data source invoked\")\n",
    "\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "\n",
    "    documents = \"The IRS, or Internal Revenue Service, is the federal agency responsible for administering and enforcing tax laws in the United States. It is a bureau of the Department of the Treasury and is responsible for collecting taxes and processing tax returns. The IRS plays a crucial role in the federal government's revenue collection efforts, ensuring that individuals and businesses pay their fair share of taxes to fund various public services and programs.\"\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents , \"question\": question}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_router(state):\n",
    "    state_dict = state[\"keys\"]\n",
    "    route = state_dict[\"route\"]\n",
    "    if route.replace(\"\\n\",\"\") == \"fixed\":\n",
    "        return \"fixed\"\n",
    "    else:\n",
    "        return \"rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Assembly\n",
    "\n",
    "This graph will consist of routing between 2 different data sources and generation. It will pick the correct data source based on the inputted query. We have queries that will be able to read the original knowledge base containing information about amazon and the fixed output string talking about the IRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"data_source_router\", router_invoke)  \n",
    "workflow.add_node(\"retrieve\", retrieve)  \n",
    "workflow.add_node(\"generate\", generate)  \n",
    "workflow.add_node(\"fixed_data\", fixed_data)  \n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"data_source_router\")\n",
    "workflow.add_conditional_edges( ## Once the router is called, the output is read by the rag_router function and based on the final route output, it will trigger the correct node\n",
    "    \"data_source_router\",\n",
    "    rag_router,\n",
    "    {\n",
    "        \"rag\": \"retrieve\",\n",
    "        \"fixed\": \"fixed_data\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"generate\") #Adding fixed steps to generate the retrieved content\n",
    "workflow.add_edge(\"fixed_data\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case to invoke the KB\n",
    "question = \"Tell me about Amazon's work in Generative AI\"\n",
    "inputs = { \"keys\":{\"question\":question}}\n",
    "graph_out = app.invoke(inputs)\n",
    "print(graph_out[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case to invoke the fixed response\n",
    "question = \"Tell me about taxes\"\n",
    "inputs = { \"keys\":{\"question\":question}}\n",
    "graph_out = app.invoke(inputs)\n",
    "print(graph_out[\"keys\"][\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
